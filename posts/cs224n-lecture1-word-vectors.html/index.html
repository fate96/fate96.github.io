<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>cs224n Lecture1 Word Vectors - Luckyouo Blog</title><meta name=Description content="Luckyouo 的小窝"><meta property="og:title" content="cs224n Lecture1 Word Vectors"><meta property="og:description" content="前言 由于 one-hot 将词转为向量的方法简单且容易实现，但生成的向量并不能表示各个词之间的关系（比如常见的余弦相似度，因为 one-hot 生成的向量两两正交），而 word2vec 算"><meta property="og:type" content="article"><meta property="og:url" content="https://luckyouo.github.io/posts/cs224n-lecture1-word-vectors.html/"><meta property="og:image" content="https://luckyouo.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-22T18:49:46+08:00"><meta property="article:modified_time" content="2022-06-14T15:45:44+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://luckyouo.github.io/logo.png"><meta name=twitter:title content="cs224n Lecture1 Word Vectors"><meta name=twitter:description content="前言 由于 one-hot 将词转为向量的方法简单且容易实现，但生成的向量并不能表示各个词之间的关系（比如常见的余弦相似度，因为 one-hot 生成的向量两两正交），而 word2vec 算"><meta name=application-name content="LoveIt"><meta name=apple-mobile-web-app-title content="LoveIt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://luckyouo.github.io/posts/cs224n-lecture1-word-vectors.html/><link rel=prev href=https://luckyouo.github.io/posts/numpy-%E5%B8%B8%E7%94%A8-api.html/><link rel=next href=https://luckyouo.github.io/posts/jwt-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"cs224n Lecture1 Word Vectors","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/luckyouo.github.io\/posts\/cs224n-lecture1-word-vectors.html\/"},"genre":"posts","keywords":"cs224n, word2vec","wordcount":1900,"url":"https:\/\/luckyouo.github.io\/posts\/cs224n-lecture1-word-vectors.html\/","datePublished":"2022-04-22T18:49:46+08:00","dateModified":"2022-06-14T15:45:44+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"luckyouo"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><a href=https://github.com/luckyouo class=github-corner target=_blank title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="3.5rem" height="3.5rem" viewBox="0 0 250 250" style="fill:#70b7fd;color:#fff;position:absolute;top:0;border:0;left:0;transform:scale(-1,1)" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div class=header-title><a href=/ title="Luckyouo Blog">Luckyouo Blogs</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>归档 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/friends/><i class='fas fa-fw fa-fan fa-spin'></i> 友链 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Luckyouo Blog">Luckyouo Blogs</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>归档</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/friends/ title><i class='fas fa-fw fa-fan fa-spin'></i>友链</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">cs224n Lecture1 Word Vectors</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>luckyouo</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/cs224n/><i class="far fa-folder fa-fw"></i>cs224n</a></span></div><div class=post-meta-line><i class="far fa-calendar fa-fw"></i>&nbsp;<time datetime=2022-04-22>2022-04-22</time>&nbsp;<i class="far fa-calendar-plus fa-fw"></i>&nbsp;<time datetime=2022-06-14>2022-06-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1900 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 4 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ol><li><a href=#前言>前言</a></li><li><a href=#内容>内容</a><ol><li><a href=#基本模型>基本模型</a></li><li><a href=#negative-sampling>Negative Sampling</a></li><li><a href=#hierarchical-softmax>Hierarchical softmax</a></li></ol></li><li><a href=#参考资料>参考资料</a></li></ol></nav></div></div><div class=content id=content><h2 id=前言>前言</h2><p>由于 one-hot 将词转为向量的方法简单且容易实现，但生成的向量并不能表示各个词之间的关系（比如常见的余弦相似度，因为 one-hot 生成的向量两两正交），而 word2vec 算法则解决了该问题，word2vec 算法主要有两种算法，一种是 <code>跳元模型</code> （Skip-grams , SG）算法，利用中心词推导上下文，另一种是 <code>连续词袋</code> （Continuous Bag of Words, CBOW）算法，利用上下文推导中心词。cs224n 第一节课主要介绍了 word2vec 跳元模型算法的原理与公式推导。</p><h2 id=内容>内容</h2><h3 id=基本模型>基本模型</h3><p>在跳元模型当中，每个词都包含一对向量，即作为<em>中心词</em>向量和<em>上下文词</em>向量，更具体地说，对于词典中索引为i的任何词，分别用 $\mathbf{u}_i\in\mathbb{R}^d$ 和 $\mathbf{u}_i\in\mathbb{R}^d$ 表示其用作<em>中心词</em>和<em>上下文词</em>时的两个向量，计算给定中心词 $w_t$ 上下文词的条件概率，得到最大似然函数。最后利用最大似然函数的特性，转换为 log 函数进行最小化求解。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_1.png data-srcset="/images/cs224n_1.png, /images/cs224n_1.png 1.5x, /images/cs224n_1.png 2x" data-sizes=auto alt=/images/cs224n_1.png title=image></p><p>现在关键的问题为求解条件概率 $P(W_{t+j} | W_t; \theta)$ 。对于每个词 $w$，定义其两个词向量：$v_w$表示当 $w$ 为中心词时的词向量，$u_w$ 表示当w为其他词的临近词时的词向量。则对于一个中心词c和其临近词 $o$，则可以通过 softmax 进行建模。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_2.png data-srcset="/images/cs224n_2.png, /images/cs224n_2.png 1.5x, /images/cs224n_2.png 2x" data-sizes=auto alt=/images/cs224n_2.png title=image-20220422201906064></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_3.png data-srcset="/images/cs224n_3.png, /images/cs224n_3.png 1.5x, /images/cs224n_3.png 2x" data-sizes=auto alt=/images/cs224n_3.png title=image-20220422202232765></p><p><strong>softmax 函数可以将放大较大 $x_i$ 概率（max），同时还保持较小的数的概率不为0（soft）。</strong></p><p>求导的梯度如下，$u_o$ 表示观察到的上下文词向量，减号后面的是这个位置的期望的词向量，期望=概率*值。差值就是真实观察词向量减去期望词向量，这就是梯度。当它们的差值很小时，说明给定 $c$ 能很好的预测其临近词的概率分布。</p><p>对中心向量求导：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_8.png data-srcset="/images/cs224n_8.png, /images/cs224n_8.png 1.5x, /images/cs224n_8.png 2x" data-sizes=auto alt=/images/cs224n_8.png title=image-20220426104810429></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_7.png data-srcset="/images/cs224n_7.png, /images/cs224n_7.png 1.5x, /images/cs224n_7.png 2x" data-sizes=auto alt=/images/cs224n_7.png title=image-20220426104638794></p><p>对上下文向量求导：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_9.png data-srcset="/images/cs224n_9.png, /images/cs224n_9.png 1.5x, /images/cs224n_9.png 2x" data-sizes=auto alt=/images/cs224n_9.png title=image-20220426105101981></p><p>上述全连接网络虽然能很方便的计算词向量，但存在两个问题：1. 网络过于庞大，参数量太多；2. 训练样本太多，每个样本都更新所有参数，训练速度慢。针对这两个问题，作者分别提出了 subsampling 和 negative sampling 的技巧。</p><p>subsampling技巧是指，每个词有一个保留概率p，以这个概率p保留其在训练数据中，以1-p删掉这个词。对于词$w_i$，其概率 $P(w_i)$ 公式如下，其中 $z(w_i)$ 是词 $w$ 的词频。概率 $p$ 和这个词在语料库中的词频有关，词频越大，保留概率越低，即被删掉的概率越大，所以subsampling之后应该能较大的减少训练数据。</p><p>$$P(W_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) * \frac{0.001}{z(w_i)}$$</p><h3 id=negative-sampling>Negative Sampling</h3><p><strong>negative sampling</strong> 技巧，只更新一小部分参数。比如对于(&ldquo;fox&rdquo;, &ldquo;quick&rdquo;)，期望的输出是quick的one-hot，即只有quick对应位为1，其他为0。但网络的softmax输出肯定不可能是完完全全的quick为1，其他为0；有可能是quick为0.8，其他位有些0.001，0.03之类的非0值，这就导致输出层的所有神经元都有误差。按照传统的方法，输出层所有神经元对应的U矩阵的权重都要更新。negative sampling技巧是，只更新和quick连接的U权重以及随机选5个输出神经元的连接权重进行更新，这样一下把需要更新的 U 权重个数从300w降到了6*300=1800，只需要更新0.06%的参数，大大减小了参数更新量！</p><p>考虑一对中心词和上下文词$(w, c)$，我们通过 $P(D = 1 | w,c)$ 表示 $(w, c)$是来自语料库。相应地，表示 $P(D = 0 | w,c)$ 不是来自语料库。首先,我们对 $P(D = 1 | w,c)$ 用 sigmoid 函数建模:</p><p>$$ P(D = 1| w, c, \theta) = \sigma(v_c^tv_w) = \frac{1}{1 + e^{-v_c^tv_w}}$$</p><p>我们建立一个新的目标函数,如果中心词和上下文词确实在语料库中，就最大化概率 ,如果中心词和上下文词确实不在语料库中，就最大化概率 $P(D = 1 | w,c)$ 。我们对这两个概率采用一个简单的极大似然估计的方法(这里我们把
$\theta$ 作为模型的参数,在我们的例子是 $V$ 和 $U$。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_10.png data-srcset="/images/cs224n_10.png, /images/cs224n_10.png 1.5x, /images/cs224n_10.png 2x" data-sizes=auto alt=/images/cs224n_10.png title=image-20220426110234357></p><p>最大化似然函数等同于最小化负对数似然：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_11.png data-srcset="/images/cs224n_11.png, /images/cs224n_11.png 1.5x, /images/cs224n_11.png 2x" data-sizes=auto alt=/images/cs224n_11.png title=image-20220426110352001></p><p>有很多关于如何得到最好近似的讨论,从实际效果看来最好的是指数为 3/4 的 Unigram 模型。</p><p>$$P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^{n} f(w_j)^{3/4}}$$</p><h3 id=hierarchical-softmax>Hierarchical softmax</h3><p>在实际中，hierarchical softmax 对低频词往往表现得更好，负采样对高频词和较低维度向量表现得更好。</p><p>Hierarchical softmax 使用一个二叉树来表示词表中的所有词。树中的每个叶结点都是一个单词,而且只有一条路
径从根结点到叶结点。在这个模型中,没有词的输出表示。相反,图的每个节点(根节点和叶结点除外)与模型要
学习的向量相关联。单词作为输出单词的概率定义为从根随机游走到单词所对应的叶的概率。计算成本变为 $O(log|V|)$ 而不是 $O(|V|)$ 。</p><p>下图是 Hierarchical softmax 的二叉树示意图</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/cs224n_12.png data-srcset="/images/cs224n_12.png, /images/cs224n_12.png 1.5x, /images/cs224n_12.png 2x" data-sizes=auto alt=/images/cs224n_12.png title=image-20220426111039969></p><p>让我们引入一些概念。令 $L(w)$ 为从根结点到叶结点 $w$ 的路径中节点数目。例如，上图中的 $L(w_2)$ 为 3。我们定
义 $n(w, i)$ 为与向量 $v_n(w, i)$ 相关的路径上第 $i$ 个结点。因此 $n(w, 1)$ 是根结点，而 $n(w, L(w))$ 是 $w$ 的父节
点。现在对每个内部节点 $n$ ，我们任意选取一个它的子节点，定义为 $ch(n)$ (一般是左节点)。然后，我们可以
计算概率为：</p><p>$$p(w|w_i) = \prod_{j=1}^{L(w) - 1}\sigma([n(w, j+1) = ch(n(w, j))] * v_{n(w,j)}^Tv_{w_j})$$</p><p>其中 $[x]$ 当 x 为true 时，为 1，否则为 0。</p><h2 id=参考资料>参考资料</h2><p><a href=https://bitjoy.net/2019/06/22/cs224n%ef%bc%881-8%ef%bc%89introduction-and-word-vectors/ target=_blank rel="noopener noreffer">CS224N（1.8）Introduction and Word Vectors</a></p><p><a href=http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ target=_blank rel="noopener noreffer">Word2Vec Tutorial - The Skip-Gram Model</a></p><p><a href=http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ target=_blank rel="noopener noreffer">Word2Vec Tutorial Part 2 - Negative Sampling</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-06-14</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/posts/cs224n-lecture1-word-vectors.html/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://luckyouo.github.io/posts/cs224n-lecture1-word-vectors.html/ data-title="cs224n Lecture1 Word Vectors" data-hashtags=cs224n,word2vec><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://luckyouo.github.io/posts/cs224n-lecture1-word-vectors.html/ data-hashtag=cs224n><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://luckyouo.github.io/posts/cs224n-lecture1-word-vectors.html/ data-title="cs224n Lecture1 Word Vectors"><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/cs224n/>cs224n</a>,&nbsp;<a href=/tags/word2vec/>word2vec</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/numpy-%E5%B8%B8%E7%94%A8-api.html/ class=prev rel=prev title="Numpy 常用 api"><i class="fas fa-angle-left fa-fw"></i>Numpy 常用 api</a>
<a href=/posts/jwt-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html/ class=next rel=next title="JWT 学习笔记">JWT 学习笔记<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><span id=run-time></span></div><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.112.5">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>luckyouo</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=sidebar_wo><div id=leimu><img src=https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/leimuA.png alt=雷姆 onmouseover='this.src="https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/leimuB.png"' onmouseout='this.src="https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/leimuA.png"' title=回到顶部></div><div class=sidebar_wo id=lamu><img src=https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/lamuA.png alt=雷姆 onmouseover='this.src="https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/lamuB.png"' onmouseout='this.src="https://cdn.jsdelivr.net/gh/lewky/lewky.github.io@master/images/b2t/lamuA.png"' title=回到底部></div></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript src=/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:15},comment:{},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/lib/lunr/lunr.segmentit.js",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js></script><script type=text/javascript src=/js/custom.js></script></body></html>