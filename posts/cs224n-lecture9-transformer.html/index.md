# cs224n lecture9 transformer


## 前言

由于注意力极值可以大幅提高 RNN 网络的性能，为此，google 提出了一个只是用注意力机制的网络。在 ` Is Attention All We Need`  paper中，提出了 Transformer，该网络没有传统的循环网络，只有注意力机制。

## Transformer

![image-20220613214451273](/images/cs224n_l10_1.png)

RNN 等循环神经网络的弊端有如下

- 不能长距离交互，虽然有 LSTM 可以防止梯度消失，但仍然存在影响作用小
- 不能并行话，由于下一个隐藏状态受到上一个状态的影响，因此必须按顺序计算，所以受到序列长度的影响

而注意力机制则不受到序列长度，可以在 $O(1)$ 的时间复杂度完成注意力计算，计算出来的分数代表对该位置元素的重要程度。

![image-20220613215102659](/images/cs224n_l9_2.png)

注意力也存在如下问题：

1. 由于注意力之间都是线性交互，只是对注意力分数的平均化，缺少非线性变化
2. 网络加深时，和 RNN 一样，会出现梯度消失问题
3. 很难训练给定层的参数，因为它来自下层的输入不断变化
4. 点积仍然倾向于取极值，因为它的方差随维度 d
5. 注意力计算时，没有考虑输入的位置信息

在 transformer 网络分别采用如下方法解决以上问题

1. 在注意力块后增加一个前反馈网络，该网络增加一个非线性变化
2. 采用残差连接，防止梯度消失，使得网络可以加深
3. 使用 Layer Norm，减少无信息变化
4. 求注意力时，使用 $\sqrt{d}$  规范化
5. 在词嵌入前，加入位置编码，将位置编码直接加如词嵌入的向量当中

在 transformer 中的位置编码采用的是 cos 编码：

![image-20220613220141870](/images/cs224n_l9_3.png)

周期性表明可能是“绝对位置”并不那么重要，且随着周期重新开始，也许可以推断出更长的序列

 同时为了让注意力学习到不同的注意方向，transformer 提出了多头注意力，每个头可以根据注意方向不同得到不同的注意力分数，并最后将其合并起来。

![image-20220613220444817](/images/cs224n_l9_4.png)

在解码部分，由于注意力是全局注意力，因此可以看到最后的结果，这会让模型的结果过拟合，因此，在解码需要使用 mask 机制，将未来的数据掩盖而防止模型作弊。

