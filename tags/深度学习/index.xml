<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>深度学习 - 标签 - Luckyouo Blog</title><link>https://luckyouo.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link><description>深度学习 - 标签 - Luckyouo Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>urutchfj@gamil.com (luckyouo)</managingEditor><webMaster>urutchfj@gamil.com (luckyouo)</webMaster><lastBuildDate>Sat, 26 Mar 2022 14:00:37 +0800</lastBuildDate><atom:link href="https://luckyouo.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="self" type="application/rss+xml"/><item><title>Transformer 代码解析</title><link>https://luckyouo.github.io/posts/transformer-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90.html/</link><pubDate>Sat, 26 Mar 2022 14:00:37 +0800</pubDate><author>作者</author><guid>https://luckyouo.github.io/posts/transformer-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90.html/</guid><description>前言 Transformer 是 2017 年 Google 团队提出的新的一种 NLP 模型，采用 Encoder-Decoder （编码器-解码器）架构，使用 self-attention 机制。 其在 seq2seq 上表现出非常鲁棒的性能，并且在 Transformer 基础上，提出了众多变</description></item></channel></rss>